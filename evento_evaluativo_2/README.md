# üìä An√°lisis Avanzado de Datos: Predicci√≥n de Ingresos (Adult Income Dataset)

Este repositorio documenta el flujo completo de un proyecto de _Data Science_ enfocado en el **Adult Income Dataset** (Censo de Ingresos de EE. UU.). El objetivo principal es predecir si un individuo gana **m√°s de $50,000 USD** al a√±o, utilizando t√©cnicas avanzadas de preprocesamiento, escalado robusto y **An√°lisis Discriminante Lineal (LDA)** para optimizar la dimensionalidad.

## üöÄ Estructura del Proyecto

El an√°lisis se divide en tres fases principales, siguiendo la convenci√≥n de _notebooks_ secuenciales, y utiliza el archivo `requirements.txt` para gestionar las dependencias.

| Archivo                               | Descripci√≥n                                                                                                                                                     | Fase del An√°lisis              |
| :------------------------------------ | :-------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------- |
| `01_exploracion_base_datos.ipynb`     | **Exploraci√≥n Inicial.** Carga y estandarizaci√≥n de nombres de columnas. Primeros chequeos de tipos de datos y estructuras.                                     | **Exploraci√≥n**                |
| `02_eda.ipynb`                        | **An√°lisis Exploratorio de Datos (EDA).** Manejo de valores faltantes (imputaci√≥n/eliminaci√≥n), an√°lisis de _outliers_ y creaci√≥n de la _feature_ `grupo_edad`. | **Limpieza y Entendimiento**   |
| `03_preprocesamiento_reduccion.ipynb` | **Preprocesamiento Avanzado y LDA.** Implementaci√≥n del `ColumnTransformer`, escalado de variables y aplicaci√≥n del LDA.                                        | **Preparaci√≥n y Optimizaci√≥n** |
| `requirements.txt`                    | Lista de librer√≠as esenciales de Python (`pandas`, `numpy`, `scikit-learn`, `seaborn`, etc.) para reproducir el entorno.                                        | **Configuraci√≥n**              |
| `adult.csv`                           | El _dataset_ original del censo de ingresos de EE. UU.                                                                                                          | **Datos Fuente**               |
| `datos_limpios.csv`                   | El _dataset_ resultante despu√©s de la fase de EDA (`02_eda.ipynb`), listo para el preprocesamiento final.                                                       | **Datos Intermedios**          |

---

## üõ†Ô∏è Configuraci√≥n y Ejecuci√≥n

Para ejecutar el proyecto localmente, aseg√∫rese de tener Python instalado y siga estos pasos:

1.  **Instalar Dependencias:**

    ```bash
    pip install -r requirements.txt
    ```

2.  **Ejecutar Notebooks:**
    Abra Jupyter Notebook o JupyterLab y ejecute los _notebooks_ en orden secuencial: `01_exploracion...`, `02_eda...`, y finalmente `03_preprocesamiento_reduccion.ipynb`.

---

### 1. Manejo de Variables Num√©ricas con Outliers (RobustScaler y Logaritmo)

Las variables con fuerte sesgo, como `ganancia_capital` y `perdidas_capital`, fueron tratadas en dos pasos cruciales:

- **Transformaci√≥n Logar√≠tmica:** Se aplic√≥ la transformaci√≥n **$\log(x+1)$** (usando `np.log1p`) para mitigar el sesgo y comprimir los valores at√≠picos extremos, que eran comunes en estas columnas.
- **Creaci√≥n de _Dummies_:** Se crearon _features_ binarias (`tiene_ganancia_capital`, `tiene_perdida_capital`) para que el modelo pudiera capturar el valor informativo del cero (la mayor√≠a de las observaciones).
- **Escalado Final:** Se utiliz√≥ el **`RobustScaler`** para escalar todas las variables num√©ricas finales. Este _scaler_ utiliza la Mediana y el Rango Intercuart√≠lico (IQR), haci√©ndolo **inmune a los _outliers_** restantes.

### 2. Codificaci√≥n Categ√≥rica

Las variables nominales como `clase_de_trabajo`, `ocupacion`, etc., fueron codificadas mediante **`OneHotEncoder`** dentro del `ColumnTransformer`, asegurando un tratamiento uniforme y sin fugas de datos.

---

## üéØ Reducci√≥n de Dimensionalidad con LDA

En el _notebook_ `03`, se eligi√≥ el **An√°lisis Discriminante Lineal (LDA)** sobre el PCA, ya que el problema es de clasificaci√≥n (`ingreso` binario).

- **Objetivo:** LDA es **supervisado** y est√° dise√±ado para encontrar el subespacio que **maximiza la separaci√≥n entre las clases** de ingreso (0: `<=50K` y 1: `>50K`).
- **Resultado:** Con dos clases, LDA reduce la dimensionalidad a **una √∫nica Componente Discriminante (LD1)**. Esta componente representa el eje de m√°xima separaci√≥n de las clases.
- **Poder Discriminatorio:** La varianza explicada por esta √∫nica componente representa su **poder discriminatorio**, que es la m√©trica clave.

### Visualizaci√≥n

La efectividad de LDA se visualiza mediante un **Gr√°fico de Densidad (KDE Plot)**, que superpone las distribuciones de ambas clases a lo largo de la Componente LD1. Una separaci√≥n clara en este gr√°fico indica que la proyecci√≥n de LDA ha sido exitosa para distinguir entre los grupos de altos y bajos ingresos.

**Autores:** Ana Maria Valencia Quintero, Juan David Gaviria Correa, Juan Sebastian Restrepo Nieto
